---
title: "ML-Agents : ML-Agent를 통한 강화 학습"
excerpt: "강화학습과 학습 모델 적용"
toc: true
toc_sticky: true
toc_label: "목차"
categories:
  - ML-Agents
tags:
  - [Unity]
date: 2021-03-25
breadcrumb: true
---


# ML-Agent를 통한 강화 학습

이전 단계에서 에이전트를 구성하고 강화학습을 스테이지 구성을 끝마쳤다.

이번 시간에는 구현한 환경을 통해 실제로 강화 학습을 진행시켜보자

### MaxStep 설정

  Agent에 추가한 AgentObj의 MaxStep 속성은 에이전트가 한 에피소드내에서 무작위로 액션을 취하는 최대 횟수를 의미한다. 설정된 횟수동안 액션을 취해도 아무런 보상이 없다면 더이상 학습을 의미가 없기에 에피소드를 종료하고 다시 시작한다. MaxStep의 수치는 환경의 복잡도에 따라서 적절한 값을 찾아야한다.

  해당 프로젝트는 간단한 환경이기 때문에 MaxStep을 1000으로 설정한다.

![/assets/images/posts/2021-03-25/ml5/Untitled.png](/assets/images/posts/2021-03-25/ml5/Untitled.png)

### Behaviour Name 설정

 해당 설정은 환경설정 파일에서 자신의 학습환경 값을 찾기 위한 속성이고 훈련이 끝난 후 해당 이름으로 모델 파일이 생성된다.

![/assets/images/posts/2021-03-25/ml5/Untitled%201.png](/assets/images/posts/2021-03-25/ml5/Untitled%201.png)

### Configuration YAML 파일 생성

  학습을 위해서 먼저 YAML 설정 파일을 생성한다. 깃으로부터 내려받은 ml-agents 폴더에서  BallAgent.yaml로 생성하고 ml-agents/config/ppo/ 폴더에 저장한다. 그리고 내용을 아래와 같이 설정한다.

```csharp
behaviors:
  BallAgent:
    trainer_type: ppo
    hyperparameters:
      batch_size: 64
      buffer_size: 2048
      learning_rate: 0.0003
      beta: 0.001
      epsilon: 0.2
      lambd: 0.99
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    keep_checkpoints: 5
    max_steps: 200000
    time_horizon: 1000
    summary_freq: 10000
    threaded: true
```

### 학습

 터미널을 열어 아까 만들어준 yaml파일을 아래코드와 같은 형식으로 실행시킨다.

```csharp
mlagents-learn {학습환경설정파일.yaml} --run-id={Behaviour 명}
```

그러면 아래와 같은 화면이 출력된다. 이후에 유니티에서 실행버튼을 눌러 학습을 시작한다.

![/assets/images/posts/2021-03-25/ml5/Untitled%202.png](/assets/images/posts/2021-03-25/ml5/Untitled%202.png)

![/assets/images/posts/2021-03-25/ml5/Untitled%203.png](/assets/images/posts/2021-03-25/ml5/Untitled%203.png)

yaml파일에서 설정한대로 총 20만번을 학습하고 10000번마다 중간 결과를 출력한다.

![/assets/images/posts/2021-03-25/ml5/Untitled%204.png](/assets/images/posts/2021-03-25/ml5/Untitled%204.png)

### 생성된 모델의 적용

  지정된 20만번의 훈련이 끝나면 유니티는 자동으로 종료되며 자동으로 생성된 result 폴더 하위에 있는 BallAgent.nn 파일을 유니티 프로젝트 뷰에 드래그하여 가져온다.

  Agent를 선택하여 임포트한 BallAgent.onnx파일과 Model 속성에 연결한 후 유니티를 실행하면 훈련된 대로 움직이는 에이전트를 확인할 수 있다.

(2019버전, python 3.8버전으로 오류가 발생) → (2020.2버전과 python3.7으로 교체 후 해결)

![/assets/images/posts/2021-03-25/ml5/Untitled%205.png](/assets/images/posts/2021-03-25/ml5/Untitled%205.png)

![/assets/images/posts/2021-03-25/ml5/Untitled%206.png](/assets/images/posts/2021-03-25/ml5/Untitled%206.png)

이것으로 간단한 강화학습을 진행하고 해당 모델을 통한 결과를 관찰하였다.

다음에 가능하다면 조금은 더 복잡한 강화학습을 진행시켜 원하는 결과를 얻고싶다.